---
weight: 50
title: Alarms generated by the Apama-ctrl microservice
layout: redirect
---

Alarms are created by user applications in the {{< product-c8y-iot >}} tenant (for example, by an analytic model, an activated EPL file, or a smart rule). To learn about alarms in general, refer to [Device Management > Working with alarms](/users-guide/device-management/#alarm-monitoring) in the *User guide*. The Apama-ctrl microservice also generates alarms because it has encountered some problem, so that the user is notified about the situation. The information below is about alarms that are generated by the Apama-ctrl microservice, their causes, consequences and possible ways to resolve them.

> **Info:** Alarms generated by Apama-ctrl about its own state are available as of Analytics Builder 10.5.0 and EPL Apps 10.5.0.
>

 You can view alarms in the following ways:

1. In the Cockpit application. See [Cockpit](/users-guide/cockpit/) in the *User guide* for detailed information.
2. In the Administration application, under **Ecosystem** > **Microservices**. Click the Apama-ctrl microservice and then click **Status**. 
   See [Administration > Managing and monitoring microservices](/users-guide/administration/#managing-microservices) in the *User guide* for detailed information.
3. From the Streaming Analytics application. Click the **Diagnostics** (or **Enhanced**) link which is provided at the bottom of the home screen. A ZIP file is then downloaded that contains alarms information under */alarm/alarms_apama-ctrl-object.json*. See [Downloading diagnostics and logs](#diagnostics-download) for detailed information.

### Alarm severities

| Severity | Description                                                  |
| -------- | ------------------------------------------------------------ |
| CRITICAL | Apama-ctrl was unable to continue running the user's applications and will require corrective action. |
| MAJOR    | Apama-ctrl has encountered a situation that will result in some loss of service (for example, due to a restart). |
| MINOR    | Apama-ctrl has a problem that you might want to fix.         |
| WARNING  | There is a warning.                                          |

### Alarms created by the Apama-ctrl microservice

Apama-ctrl can create alarms to notify users in scenarios such as the correlator running out of memory, uncaught exceptions in activated EPL files, and so on. Once you see an alarm in the {{< product-c8y-iot >}} tenant, you should diagnose it and resolve it depending on the severity level of the raised alarm. Each alarm has details such as title, text, type, date, and count (represents the number of times the alarm has been raised).

The following is a list of the alarms. The information further down below explains when these alarms will occur, their consequences, and how to resolve them.

- [Change in tenant options and restart of Apama-ctrl](#tenant_option_change)
- [Safe mode on startup](#apama_safe_mode)
- [Deactivating models in Apama Starter](#apama_ctrl_starter)
- [High memory usage](#apama_highmemoryusage)
- [Warning or higher level logging from an EPL file](#apama_ctrl_fatalcritwarn)
- [An EPL file throws an uncaught exception](#apama_ctrl_error)
- [An EPL file blocks the correlator context for too long](#apama_ctrl_warn)
- [EPL app restore timeout on restart of Apama-ctrl](#eplapp_restore_timeout)
- [Multiple extensions with the same name](#extension_error)
- [Smart rule configuration failed](#smartrule_configuration_error)
- [Smart rule restore failed](#smartrule_restore_failed)
- [Connection to correlator lost](#lost_correlator_connection)
- [The correlator queue is full](#application_queue_full)
- [The CEP queue is full](#cep_queue_full) (this alarm is coming from {{< product-c8y-iot >}} Core, but concerns Apama-ctrl)

Once the cause of an alarm is resolved, you must acknowledge and clear the alarm in the {{< product-c8y-iot >}} tenant. Otherwise, you will continue to see the alarm until a further restart of the Apama-ctrl microservice.

> **Info:** The alarm texts for the alarms below may undergo minor changes in the future.

<a name="tenant_option_change"></a>
#### Change in tenant options and restart of Apama-ctrl

This alarm is raised when a tenant option changes in the `analytics.builder` or `streaminganalytics` category. For details on the tenant options, refer to the [Tenant API](https://{{< domain-c8y >}}/api/{{< c8y-current-version >}}/#tag/Tenant-API) in the {{< openapi >}} for more details.

- Alarm type: `tenant_option_change`
- Alarm text: Apama detected changes in tenant option. Apama will restart in order to use it.
- Alarm severity: MAJOR

Analytics Builder allows you to configure its settings by changing the tenant options, using key names such as `numWorkerThreads` or `status_device_name`. For example, if you want to process things in parallel, you can set `numWorkerThreads` to 3 by sending a REST request to {{< product-c8y-iot >}}, which will update the tenant option. Such a change automatically restarts the Apama-ctrl microservice. To notify the users about the restart, Apama-ctrl raises an alarm, saying that Apama has detected a change in a tenant option and will restart in order to use it.

Once you see this alarm, you can be sure that your change is effective.

<a name="apama_safe_mode"></a>
#### Safe mode on startup

This alarm is raised whenever the Apama-ctrl microservice switches to safe mode.

- Alarm type: `apama_safe_mode`
- Alarm text: Apama appears to be repeatedly restarting. As a precaution, user-provided EPL, analytic models and extensions that might have caused this have been disabled. Refer to the audit log for more details. Please check any recent alarms, or contact support or your administrator.
- Alarm severity: CRITICAL

Apama detects if it has been repeatedly restarting. If it looks like this has been caused by any kind of user asset (EPL, analytic models, extensions), Apama disables them all as a precaution. Potential causes are, for example, an EPL app that consumes more memory than is available or an extension containing bugs.

You can check the mode of the microservice (either normal or safe mode) by making a REST request to *service/cep/diagnostics/apamaCtrlStatus* (available as of EPL Apps 10.5.7 and Analytics Builder 10.5.7), which contains a `safe_mode` flag in its response.

To diagnose the cause of an unexpected restart, you can try the following:

- Check the EPL apps memory profiler by making a REST request to */service/cep/diagnostics/eplMemoryProfiler* (available as of EPL Apps 10.5.7) for any memory leaks.

    Note that you must re-activate the EPL apps that were active before as the Apama-ctrl microservice loses information about the previous microservice instance when it restarts due to safe mode. To replicate the previous scenario, run the EPL apps and process some events to trigger a leak and then use the memory profiler to check for any memory leaks.

- Check the microservice logs for any exceptions by downloading the diagnostics overview ZIP file as described in [Downloading diagnostics and logs](#diagnostics-download). In the downloaded ZIP file, you can find the logs under */diagnostics/*.

    As mentioned in the above point, re-activate the EPL apps and analytic models that were active before and then check the logs.

- Check the audit logs.

In safe mode, all previously active analytic models and EPL apps are deactivated and must be manually re-activated.

<a name="apama_ctrl_starter"></a>
#### Deactivating models in Apama Starter

This alarm is raised when Apama-ctrl switches from the fully capable microservice to Apama Starter with more than 3 active models.

- Alarm type: `apama_ctrl_starter`
- Alarm text: The following models were de-activated as Apama Starter is restricted to 3 active models: (&lt;models&gt;).
- Alarm severity: MINOR

In Apama Starter, a user can have a maximum of 3 active models. For example, a user is working with the fully capable Apama-ctrl microservice and has 5 active models, and then switches to Apama Starter. Since Apama Starter does not allow more than 3 active models, it deactivates all the active models (5) and raises an alarm to notify the user.

<a name="apama_highmemoryusage"></a>
#### High memory usage

This alarm is raised whenever the correlator consumes 90% of the maximum memory permitted for the microservice container. During this time, the Apama-ctrl microservice automatically generates the diagnostics overview ZIP file which contains diagnostics information used for identifying the most likely cause for memory consumption.

There are 3 variants of this alarm, depending on the time and count restrictions of the generated diagnostics overview ZIP file.

First variant:

- Alarm type: `apama_highmemoryusage`
- Alarm text: Apama is using 90% of available memory (&lt;totalMemory&gt;). Your apps will be in danger of crashing. Diagnostics file is located at &lt;URL-to-ZIP-file&gt; You can also download the file by navigating to Administration > Management > Files Repository
- Alarm severity: WARNING

Second variant:

- Alarm type: `apama_highmemoryusage`
- Alarm text: Apama is using 90% of available memory (&lt;totalMemory&gt;). Your apps will be in danger of crashing. Have recently created diagnostics snapshot (within last hour).
- Alarm severity: WARNING

Third variant:

- Alarm type: `apama_highmemoryusage`
- Alarm text: Apama is using 90% of available memory (&lt;totalMemory&gt;). Your apps will be in danger of crashing. Have created 5 diagnostics snapshots, not creating any more, refer to past alarms.
- Alarm severity: WARNING

Running EPL apps (and to a lesser extent, smart rules and analytic models) consumes memory, the amount will depend a lot on the nature of the app running. The memory usage should be approximately constant for a given set of apps, but it is possible to create a "memory leak", particularly in an EPL file or a custom block. The Apama-ctrl microservice monitors memory and raises an alarm with WARNING severity if the 90% memory limit is reached along with the diagnostics overview ZIP file and saves it to the files repository (as mentioned in the alarm text).

Apama-ctrl generates the diagnostics overview ZIP files with the following conditions:

- Only if it has not been generated in the last 1 hour.
- A maximum of 5 diagnostics overview ZIP files from its start time until it stops.
- Overall, it can generate a maximum of 20 ZIP files per {{< product-c8y-iot >}} tenant, beyond which it keeps deleting the oldest ZIP files during its startup process.

To diagnose high-memory-consuming models and EPL apps, you can try the following (it could be listener leaks, excessive state being stored or spawned monitors leaking, and so on):

- Download the automatically generated diagnostics overview ZIP file (refer to the alarm text for its location) and look at *correlator/inspect.json* and *correlator/status.json* for the number of listeners (this number may be large in the case of a listener leak). Note that this is only an overview and excludes the EPL memory profile output.

- Download diagnostics information from the Streaming Analytics application using the **Diagnostics** link (as described in [Downloading diagnostics and logs](#diagnostics-download)). When using the **Enhanced** link, the diagnostics information includes (in addition to the information that you get with the **Diagnostics** link) requests that are more resource-intensive and may significantly slow down the correlator, including EPL memory profiler snapshots and the contents of the queues. So when diagnosing the cause for the first time, it is recommended to use the overview ZIP file from the **Diagnostics** link, unless additional information is required.

- The EPL memory profiler from the above **Enhanced** link in */diagnostics/eplMemoryProfiler.csv* gives the memory consumed by each monitor along with details such as the number of listeners or the number of monitor instances running something like shown in the snippet below. This can help you to understand which monitor is consuming more memory and try to reduce it.

    | Monitor | Monitor instances | EPL objects | Listeners | Bytes   | Overhead bytes |
    | ------- | ----------------- | ----------- | --------- | ------- | -------------- |
    | mon1    | 1                 | 5384        | 4         | 1073908 | 383240         |
    | mon2    | 1                 | 2           | 2         | 696     | 2280           |
    | mon3    | 1                 | 4           | 1         | 840     | 752            |

- Also check for memory usage on all the input and output queues available from the **Enhanced** link in */diagnostics/toStringQueues.txt*.

If the memory continues to grow, then when it reaches the limit, the correlator will run out of memory and Apama-ctrl will shut down. To prevent the microservice from going down, you must fix this as a priority.

See also [Diagnostic tools for Apama in Cumulocity IoT](https://techcommunity.softwareag.com/techniques-blog/-/blogs/apama-in-cumulocity-iot) in {{< company-sag >}}'s {{< sag-dev-community >}}.

<a name="apama_ctrl_fatalcritwarn"></a>
#### Warning or higher level logging from an EPL file

This alarm is raised whenever messages are logged by Apama EPL files with specific log levels (including CRITICAL, FATAL, ERROR and WARNING).

The Streaming Analytics application allows you to deploy EPL files to the correlator. The Apama-ctrl microservice analyzes logged content in the EPL files and raises an alarm for specific log levels with details such as monitor name, log text and alarm type (either of WARNING or MAJOR), based on the log level.

For example, the following is a simple monitor which prints a sequence and logs some texts at different EPL log levels.

```java
monitor Sample{
   action onload() {
      log "Info"; // default log level is now INFO
      log "Fatal Error" at FATAL; // log level is FATAL
      log "Critical Error" at CRIT; // log level is CRITICAL
      log "Warning" at WARN; // log level is WARNING
   }
}
```

Apama-ctrl analyzes all the log messages, filters out only certain log messages, and raises an alarm for the identified ones. Thus, Apama-ctrl generates the following three alarms for the above example:

First alarm:

- Alarm type: `APAMA_CTRL_FATAL_<HASHCODE>`
- Alarm text: &lt;Monitor name&gt;-Fatal Error.
- Alarm severity: MAJOR

Second alarm:

- Alarm type: `APAMA_CTRL_CRIT_<HASHCODE>`
- Alarm text:&lt;Monitor name&gt;-Critical Error.
- Alarm severity: MAJOR

Third alarm:

- Alarm type: `APAMA_CTRL_WARN_<HASHCODE>`
- Alarm text: &lt;Monitor name&gt;-Warning.
- Alarm severity: WARNING

<a name="apama_ctrl_error"></a>
#### An EPL file throws an uncaught exception

You have seen that the Apama-ctrl microservice raises alarms for logged messages. In addition, there can also be uncaught exceptions (during runtime). Apama-ctrl identifies such exceptions and raises alarms so that you can identify and fix the problem.

For example, the following monitor throws `IndexOutOfBoundsException` during runtime:

```java
monitor Sample{
   sequence<string> values := ["10", "20", "30"];
   action onload() {
      // IndexOutOfBoundsException (runtime error)
      log "Value = " + values[10] at ERROR;
   }
}
```

Apama-ctrl generates the following alarm for the above example:

- Alarm type: `APAMA_CTRL_ERROR_<HASHCODE>`
- Alarm text: &lt;Monitor name&gt;-Error on line &lt;x&gt; of monitor : IndexOutOfBoundsException - Out of bounds index passed to sequence [] operator - Sample
- Alarm severity: MAJOR

You can diagnose the issue by the monitor name and line number given in the alarm.

For more details, you can also check the Apama logs if the tenant has the "microservice hosting" feature enabled. Alarms of this type should be fixed as a priority as these uncaught exceptions will terminate the execution of that monitor instance, which will typically mean that your app is not going to function correctly. This might even lead to a correlator crash if not handled properly.

<a name="apama_ctrl_warn"></a>
#### An EPL file blocks the correlator context for too long

If an EPL app has an infinite loop, it may block the correlator context for too long, not letting any other apps run in the same context or, even worse, causes excessive memory usage (as the correlator is unable to perform any garbage collection cycles) leading to the app running out of memory. The Apama-ctrl microservice identifies such scenarios (the correlator logs warning messages if an app is blocking a context for too long) and raises alarms, so that the user can identify and fix the problem.

For example, the following monitor blocks the correlator main context:

```java
event MyEvent {
}

monitor Sample{
    action onload() {
        while true {
            // do something
            send MyEvent() to "foo";
        }
    }
}
```

Apama-ctrl generates the following alarm for the above example:

- Alarm type: `APAMA_CTRL_WARN_<HASHCODE>`
- Alarm text: &lt;EPLAppName&gt;.&lt;monitorName&gt; - context '&lt;contextName&gt;' has been processing a single event for a long time.
- Alarm severity: WARNING

You can diagnose the issue by the monitor name and context name given in the alarm.

For more details, you can also check the Apama logs if the tenant has the "microservice hosting" feature enabled. Alarms of this type should be fixed as a priority as these scenarios may lead to the microservice and correlator running out of memory.

<a name="eplapp_restore_timeout"></a>
#### EPL app restore timeout on restart of Apama-ctrl

If restoring an EPL app on a restart of the Apama-ctrl microservice takes a long time and exceeds the time limit
specified by the `recovery.timeoutSecs` tenant option (in the `streaminganalytics` category) or a default of 60 seconds,
the Apama-ctrl microservice times out and raises an alarm, indicating that it will restart and reattempt to restore the EPL app.
The alarm text includes the names of any EPL apps that are considered to be the reason for the timeout.

- Alarm type: `eplapp_restore_timeout`
- Alarm text: Restoring EPL apps after Apama-ctrl microservice restart has timed out. The EPL app &lt;app name&gt; could not be restored.
The following EPL apps may be the cause of this: &lt;comma-separated list of app names&gt;.
The Apama-ctrl microservice will restart now, and restoring will be reattempted.
If this continues to fail, the Apama-ctrl microservice will enter safe mode, disabling all EPL apps.
- Alarm severity: MAJOR

The following information is only included in the alarm text if the Apama-ctrl microservice detects that the timeout is due to some EPL apps:
"The following EPL apps may be the cause of this: &lt;comma-separated list of app names&gt;.".
If no such apps are detected, this information is omitted from the alarm text.


<a name="extension_error"></a>
#### Multiple extensions with the same name

This alarm is raised when the Apama-ctrl microservice tries to activate the deployed extensions during its startup process and there are multiple extensions with the same name.

- Alarm type: `extension_error`
- Alarm text: Multiple extensions with the same name have been found: &lt;list of all duplicate extension names&gt;
- Alarm severity: CRITICAL

This disables all extensions that were deployed to Apama-ctrl. In order to use the deployed extensions, the user must decide which extensions to keep and then delete the duplicate ones.

**Info:** In case of multiple duplicates, this alarm is only listed once.

<a name="smartrule_configuration_error"></a>
#### Smart rule configuration failed

This alarm is raised if a smart rule contains an invalid configuration.

- Alarm type: `smartrule_configuration_error`
- Alarm text: &lt;Smart rule identifier&gt;: Smart rule create/edit failed. One or more fields are invalid, please check smart rule configuration.
- Alarm severity: MAJOR

To diagnose the cause, download the diagnostics overview ZIP file as described in [Downloading diagnostics and logs](#diagnostics-download). Or, if that fails, log on as an administrator and look at the result of a GET request to */service/smartrule/smartrules?withPrivateRules=true*. Review the smart rules JSON and look for invalid smart rule configurations. Such smart rules must be corrected.

The Apama microservice log contains more details on the reason for the smart rule configuration failure. For example, it is invalid to configure an "On measurement threshold create alarm" smart rule with a data point that does not exist.

<a name="smartrule_restore_failed"></a>
#### Smart rule restore failed

This alarm is raised if a corrupt smart rule is present in the inventory and the correlator therefore fails to recover it correctly during startup.

- Alarm type: `smartrule_restore_failed`
- Alarm text: Smart rule restore failed. Contact support.
- Alarm severity: MAJOR

To diagnose the cause, download the diagnostics overview ZIP file as described in [Downloading diagnostics and logs](#diagnostics-download). Or, if that fails, log on as an administrator and look at the result of a GET request to */service/smartrule/smartrules?withPrivateRules=true*. Review the smart rules JSON and look for invalid smart rule configurations. Such smart rules may need to be deleted or corrected.

<a name="lost_correlator_connection"></a>
#### Connection to correlator lost

This alarm is raised in certain cases when the connection between the Apama-ctrl microservice and the correlator is lost. This should not happen, but can be triggered by high load situations.

- Alarm type: `lost_correlator_connection`
- Alarm text: Unable to ping correlator: &lt;message&gt;, Apama-ctrl will restart.
- Alarm severity: MAJOR

Apama-ctrl will automatically restart. Report this to [product support](/welcome/contacting-support) if this is happening frequently.

<a name="application_queue_full"></a>
#### The correlator queue is full

This alarm is raised whenever the correlator queue is full, including both input and output queues.

- Alarm type: `application_queue_full`
- Alarm text: InputQueueSize: &lt;size of input queue&gt;, OutputQueueSize: &lt;size of output queue&gt;, SlowestReceiver: &lt;name of the slowest receiver&gt;, SlowestReceiverQueueSize: &lt;size of slowest receiver's queue&gt;, SlowestContext: &lt;name of context with maximum pending events&gt;, SlowestContextQueueSize: &lt;size of slowest context's queue&gt;
- Alarm severity: CRITICAL

The correlator's input and output queues are periodically monitored to check for building up of events. If the pending queue size grows above the normal threshold (20,000 for the input queue and 10,000 for the output queue), an alarm is raised. The alarm text contains a snapshot of the correlator status at the time of raising the alarm. A correlator with a full input or output queue can cause a serious performance degradation.

The correlator queue size is based on the number of events, not raw bytes.

Check the alarm text to get an indication of which queue is blocking. This also contains information about the slowest receiver and the most backed-up context. To diagnose the cause, see the information given in [The CEP queue is full](#cep_queue_full). A problem is likely to trigger the "correlator queue is full" alarm followed by the "CEP queue is full" alarm.

<a name="cep_queue_full"></a>
#### The CEP queue is full

This alarm is raised whenever the CEP queue for the respective tenant is full.

- Alarm text: Real-time event processing is currently overloaded and may stop processing your events. Please contact support.
- Alarm severity: CRITICAL

Karaf nodes that send events to the CEP engine maintain per-tenant queues for the incoming events. This data gets processed by the CEP engine for the hosted CEP rules. For various reasons, these queues can become full and cannot accommodate newly arriving data. In such cases, an alarm is sent to the platform so that the end users are notified about the situation.

If the CEP queue is full, older events are removed to handle new incoming events. To avoid this, you must diagnose the cause of the queue being full and resolve it as soon as possible.

The CEP queue size is based on the number of CEP events, not raw bytes.

To diagnose the cause, you can try the following. It may be that the Apama-ctrl microservice is running slow because of time-consuming rules in the script, or the microservice is deprived of resources, or code is not optimized, and so on. Check the input and output queues from the "correlator queue is full" alarm (or from the microservice logs or from the diagnostics overview ZIP file under */correlator/status.json*).

- If both input and output queues are full, this suggests a slow receiver, possibly EPL sending too many requests (or too expensive a request) to {{< product-c8y-iot >}}.
- Else, if only the input queue is full, EPL is probably running in a tight loop. Try analyzing the *cpuProfile.csv* output in the diagnostic overview ZIP file, especially the monitor name and CPU time. The data collected in the profiler may also help in identifying other possible bottlenecks. For details, refer to [Using the CPU profiler]({{< link-apama-webhelp >}}index.html#page/apama-webhelp%2Fta-DepAndManApaApp_using_the_cpu_profiler.html) in the Apama documentation.
- Else, the cause may be some issue with connectivity or in {{< product-c8y-iot >}} Core.
